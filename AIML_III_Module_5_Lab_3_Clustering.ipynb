{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS85GhogZmSeOX84joOkRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_5_Lab_3_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Students Internship Batch of 2024**\n",
        "# **Clustering**"
      ],
      "metadata": {
        "id": "DDYG5PhfDjMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means\n",
        "\n",
        "K-Means algorithm is a centroid based clustering algorithm where the sum of distances of points from the centroid of each cluster is minimized. The final output is a set of K clusters .ie. the cluster assigned to each point and the K centroids of the clusters.\n",
        "\n",
        "### The Algorithm\n",
        "> 1.   Initialize K centroids to K points randomly and set each point's initial cluster as the centroid closest to it.\n",
        "2.   While the clusers are changing - \\\\\n",
        "a. Assign the new centroids as the centroids of the points which have the same assigned cluster. \\\\\n",
        "b. Assign the new clusters to the points as the closest centroid.\n",
        "3. Return the assignments and the centroids."
      ],
      "metadata": {
        "id": "89XtgH2FDor9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Points to think about\n",
        "Is feature scaling essential for KMeans as it is for most ML algos? Explain.\n",
        "\n",
        "\n",
        "What are ways to prevent initialization variation in KMeans?\n",
        "\n",
        "What is the training and testing complexity of KMeans? Give examples whereever needed\n",
        "\n",
        "### 1. Is Feature Scaling Essential for K-Means as It Is for Most ML Algorithms?\n",
        "\n",
        "Yes, feature scaling is essential for K-Means clustering. K-Means uses the Euclidean distance to assign points to clusters, and if the features have different scales, it can bias the clustering results towards features with larger ranges. For example:\n",
        "\n",
        "- Suppose you have two features: `height (cm)` ranging from 150-200, and `weight (kg)` ranging from 50-100. Without scaling, the `height` feature will dominate the distance calculations, leading to distorted clusters.\n",
        "- By scaling (e.g., using standardization or normalization), all features will have similar ranges, ensuring that each feature contributes equally to the distance computation.\n",
        "\n",
        "### 2. What Are Ways to Prevent Initialization Variation in K-Means?\n",
        "\n",
        "Initialization variation in K-Means can cause different results due to the random selection of initial centroids. Here are some ways to prevent or reduce this variation:\n",
        "\n",
        "- **K-Means++ Initialization**: This method improves the selection of initial centroids by spreading them out in the dataset. It selects the first centroid randomly and then chooses subsequent centroids with a probability proportional to the squared distance from the closest existing centroid. This technique increases the chances of converging to a better solution.\n",
        "\n",
        "- **Multiple Runs and Averaging**: Running the K-Means algorithm multiple times with different random initializations and selecting the clustering result with the lowest within-cluster sum of squares (WCSS) can improve consistency.\n",
        "\n",
        "- **Fixed Random Seed**: Setting a random seed (e.g., `np.random.seed(0)`) ensures that the same random numbers are generated across different runs, leading to the same initial centroids.\n",
        "\n",
        "- **Hierarchical Clustering Initialization**: Using the centroids from a different clustering method, such as hierarchical clustering, as the initial centroids can provide a good starting point.\n",
        "\n",
        "### 3. What Is the Training and Testing Complexity of K-Means? Give Examples Wherever Needed\n",
        "\n",
        "The time complexity of K-Means depends on three main factors:\n",
        "- \\(n\\): Number of data points\n",
        "- \\(k\\): Number of clusters\n",
        "- \\(d\\): Number of dimensions (features)\n",
        "\n",
        "#### Training Complexity\n",
        "- **Per Iteration Complexity**: The complexity of assigning points to clusters is \\(O(n \\times k \\times d)\\), and updating the centroids also has a complexity of \\(O(n \\times d)\\). Hence, each iteration has a complexity of approximately \\(O(n \\times k \\times d)\\).\n",
        "\n",
        "- **Total Complexity**: If the algorithm runs for a maximum of \\(I\\) iterations, the total complexity will be \\(O(I \\times n \\times k \\times d)\\).\n",
        "\n",
        "- **Example**: If you have a dataset with 10,000 points (n=10,000), 5 clusters (k=5), and 20 features (d=20), and the algorithm converges in 10 iterations (I=10), then the overall complexity would be \\(O(10 \\times 10,000 \\times 5 \\times 20)\\), which equals \\(O(10,000,000)\\).\n",
        "\n",
        "#### Testing Complexity\n",
        "K-Means does not explicitly have a \"testing\" phase as in supervised learning. However, assigning a new data point to a cluster requires computing the distance to all \\(k\\) centroids, resulting in a complexity of \\(O(k \\times d)\\) per new point.\n",
        "\n",
        "### Summary\n",
        "- **Feature Scaling**: Essential for preventing dominance by features with larger ranges.\n",
        "- **Initialization Variation**: Can be reduced using techniques like K-Means++, fixed random seeds, and multiple runs.\n",
        "- **Complexity**: Training complexity is \\(O(I \\times n \\times k \\times d)\\), and testing (assignment of new points) complexity is \\(O(k \\times d)\\)."
      ],
      "metadata": {
        "id": "-NZQ233dHMs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering\n",
        "\n",
        "KMeans is an iterative process. It will keep on running until the centroids of newly formed clusters do not change or the maximum number of iterations are reached.\n",
        "\n",
        "But there are certain challenges with K-means. It always tries to make clusters of the same size. Also, we have to decide the number of clusters at the beginning of the algorithm. Ideally, we would not know how many clusters should we have, in the beginning of the algorithm and hence it a challenge with K-means.\n",
        "\n",
        "This is a gap hierarchical clustering bridges with aplomb. It takes away the problem of having to pre-define the number of clusters. Sounds like a dream! So, let’s see what hierarchical clustering is and how it improves on K-means."
      ],
      "metadata": {
        "id": "-TJ1phcmHfzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excercises\n",
        "\n",
        "What is the need for hierarchical clustering and its advantages over KMeans?\n",
        "\n",
        "What is the advantages of Density Based Clustering over KMeans?"
      ],
      "metadata": {
        "id": "iZBKNAiZHy48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Need for Hierarchical Clustering and Its Advantages Over K-Means\n",
        "\n",
        "**Hierarchical Clustering** is a method of cluster analysis that seeks to build a hierarchy of clusters. It is particularly useful when the number of clusters is not known in advance and when a hierarchical structure is desired in the data.\n",
        "\n",
        "#### Advantages of Hierarchical Clustering Over K-Means\n",
        "\n",
        "1. **No Need to Specify the Number of Clusters**:\n",
        "   - **Hierarchical Clustering**: You don’t need to specify the number of clusters beforehand. The algorithm creates a dendrogram, which allows you to choose the number of clusters based on the hierarchy.\n",
        "   - **K-Means**: Requires the user to define the number of clusters \\(k\\) in advance.\n",
        "\n",
        "2. **Hierarchical Structure**:\n",
        "   - **Hierarchical Clustering**: Provides a tree-like structure (dendrogram) that shows the relationships between clusters, making it easy to visualize and interpret the data.\n",
        "   - **K-Means**: Does not provide any information about the relationships between clusters.\n",
        "\n",
        "3. **Flexibility with Cluster Shapes**:\n",
        "   - **Hierarchical Clustering**: Can capture complex shapes and structures in the data, as it does not assume spherical clusters.\n",
        "   - **K-Means**: Assumes clusters are spherical and equally sized, which can be limiting.\n",
        "\n",
        "4. **Robust to Outliers**:\n",
        "   - **Hierarchical Clustering**: Can be less sensitive to outliers compared to K-Means, depending on the linkage method used.\n",
        "   - **K-Means**: Can be heavily influenced by outliers, which can skew the centroid position.\n",
        "\n",
        "5. **Multiple Linkage Criteria**:\n",
        "   - **Hierarchical Clustering**: Offers various linkage criteria (e.g., single-linkage, complete-linkage, average-linkage) to define how clusters are merged.\n",
        "   - **K-Means**: Lacks this flexibility as it only minimizes the distance to centroids.\n",
        "\n",
        "### 2. Advantages of Density-Based Clustering Over K-Means\n",
        "\n",
        "**Density-Based Clustering** (e.g., DBSCAN) is a clustering method that groups together points that are close to each other based on a distance measurement and a minimum number of points.\n",
        "\n",
        "#### Advantages of Density-Based Clustering Over K-Means\n",
        "\n",
        "1. **Ability to Find Arbitrarily Shaped Clusters**:\n",
        "   - **Density-Based Clustering**: Can discover clusters of arbitrary shapes (e.g., crescent shapes, elongated shapes) based on density, making it suitable for real-world data distributions.\n",
        "   - **K-Means**: Assumes clusters are spherical and does not perform well with non-globular shapes.\n",
        "\n",
        "2. **Handling Noise and Outliers**:\n",
        "   - **Density-Based Clustering**: Naturally identifies noise points as outliers, which can be treated as separate from any cluster. This is particularly useful in datasets with noise.\n",
        "   - **K-Means**: Outliers can significantly impact the placement of centroids, leading to incorrect clustering results.\n",
        "\n",
        "3. **No Need to Specify the Number of Clusters**:\n",
        "   - **Density-Based Clustering**: Similar to hierarchical clustering, it does not require specifying the number of clusters in advance. It determines clusters based on density.\n",
        "   - **K-Means**: Requires a predefined number of clusters \\(k\\).\n",
        "\n",
        "4. **Robustness to Varying Densities**:\n",
        "   - **Density-Based Clustering**: Can effectively identify clusters of varying densities and sizes, which is useful for datasets where clusters are not evenly distributed.\n",
        "   - **K-Means**: Struggles with clusters that vary significantly in density, as it relies on distance to centroids.\n",
        "\n",
        "5. **Scalability**:\n",
        "   - **Density-Based Clustering**: Efficient for large datasets, especially with implementations like DBSCAN, which have linear time complexity in many cases.\n",
        "   - **K-Means**: Although also efficient, it can become less effective as the number of clusters \\(k\\) increases, especially with high-dimensional data.\n",
        "\n",
        "### Summary\n",
        "- **Hierarchical Clustering** provides a flexible approach to cluster analysis without requiring a pre-defined number of clusters and reveals relationships through its dendrogram structure.\n",
        "- **Density-Based Clustering** excels in identifying arbitrarily shaped clusters, handling noise and outliers effectively, and adapting to varying densities without a need for specifying the number of clusters in advance."
      ],
      "metadata": {
        "id": "5pxATbZKH6pU"
      }
    }
  ]
}