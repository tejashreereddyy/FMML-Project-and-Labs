{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZqupiSNks1YnyUU5EY1LJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_2_Lab_4_t_SNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "\n",
        "Lab 4: t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "Exercise:\n",
        "\n",
        "Now that you understand t-SNE a bit better, can you point out some differences between PCA and t-SNE. What are the advantages/disadvantages of one over the other?\n",
        "Pointers:\n",
        "a. Which of the two algorithms is linear and which one is non-linear?\n",
        "\n",
        "b. How does the non-linearity in one of these two algorithms help in capturing certain data sets?\n",
        "\n",
        "c. PCA is known to keep points which were further apart in the higher dimension, far apart in the lower dimension as well. Does t-SNE do the same? Or does it try to preserve local neighbourhood?\n",
        "\n",
        "d. Can you comment on which one of the two is computationally more expensive?\n",
        "\n",
        "How does the computational complexity and runtime of t-SNE scale with dataset size and dimensionality?\n",
        "\n",
        "What are some limitations or potential pitfalls to be aware of when using t-SNE? (tell atleast 3)\n",
        "\n",
        "Differences between PCA and t-SNE\n",
        "a. Linearity vs. Non-Linearity:\n",
        "\n",
        "PCA (Principal Component Analysis): Linear\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding): Non-linear\n",
        "b. Non-linearity and Data Sets:\n",
        "\n",
        "t-SNE's Non-Linearity: Helps in capturing complex structures in data that cannot be represented by linear relationships. For instance, it can effectively reveal clusters and patterns in data that are non-linear in nature, such as manifold structures.\n",
        "c. Preservation of Distance and Neighborhood:\n",
        "\n",
        "PCA: Aims to keep points that were far apart in the higher-dimensional space far apart in the lower-dimensional space. It preserves global structures and tries to maximize the variance captured in the first few principal components.\n",
        "t-SNE: Focuses on preserving local neighborhoods, meaning it tries to keep similar points close together in the lower-dimensional space. It is more effective in revealing local clusters and structures.\n",
        "d. Computational Expense:\n",
        "\n",
        "PCA: Computationally less expensive. It involves eigenvalue decomposition or singular value decomposition (SVD), which are relatively faster.\n",
        "\n",
        "t-SNE: Computationally more expensive due to its iterative optimization process. It involves calculating pairwise similarities and performing gradient descent, which scales poorly with the size of the dataset.\n",
        "\n",
        "Computational Complexity and Runtime of t-SNE\n",
        "Complexity:\n",
        "The computational complexity of t-SNE is O(N^2) in terms of both memory and time, where N is the number of data points. This makes it challenging to use for very large datasets.\n",
        "\n",
        "Scaling: As the dataset size and dimensionality increase, the runtime of t-SNE increases significantly. For very large datasets, approximate methods like Barnes-Hut t-SNE or other optimizations are used to reduce the computational burden.\n",
        "\n",
        "Limitations and Potential Pitfalls of t-SNE\n",
        "\n",
        "Scalability: As mentioned, t-SNE is computationally expensive and scales poorly with large datasets. This can make it impractical for very large datasets without optimizations.\n",
        "\n",
        "Parameter Sensitivity: t-SNE has several hyperparameters, such as perplexity and learning rate, which can significantly influence the outcome. It often requires careful tuning and experimentation to get meaningful results.\n",
        "\n",
        "Interpretability: While t-SNE is excellent for visualization and revealing local structures, it is not straightforward to interpret the resulting low-dimensional embedding quantitatively. The axes in t-SNE plots do not have a clear meaning, and distances are not always directly comparable.\n",
        "\n",
        "Global Structure: t-SNE is primarily designed to preserve local structures, which means that the global structure of the data may not be well-represented. This can sometimes lead to misleading interpretations if one is not careful.\n",
        "\n",
        "Random Initialization: The algorithm's results can vary with different initializations due to its stochastic nature. This variability can make it challenging to reproduce results consistently without fixing random seeds.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ATRL_AshFmE1"
      }
    }
  ]
}