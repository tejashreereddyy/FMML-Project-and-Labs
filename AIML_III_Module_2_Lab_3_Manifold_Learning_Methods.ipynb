{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX+/i5W0Pwq5/YXLvqSfiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_2_Lab_3_Manifold_Learning_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "\n",
        "Lab 3: Manifold Learning Methods\n",
        "\n",
        "Exercises!!\n",
        "How do you think would the number of neighbors effect the Isomap algorithm? What happens when the number of neighbors considered is very large? What happens when it is very low?\n",
        "\n",
        "When is the ISOMAP algorithm superior to PCA?\n",
        "\n",
        "Search up another manifold learning methods other than ISOMAP? If there are any, whats the key difference ?\n",
        "\n",
        "Suggest ways to deal with missing data in manifold learning.\n",
        "\n",
        "The number of neighbors in the Isomap algorithm significantly affects its performance and the quality of the embedded results. Here's a breakdown of how the number of neighbors influences Isomap:\n",
        "\n",
        "Effect of Number of Neighbors\n",
        "\n",
        "When the number of neighbors is very large:\n",
        "\n",
        "Dense Graph Structure: The graph becomes denser, capturing more global structure of the data.\n",
        "Increased Computational Complexity: This can make the algorithm slower and more computationally intensive.\n",
        "Potential Over-smoothing: The embedding might lose fine details of the local structure, making it harder to distinguish between closely related points.\n",
        "When the number of neighbors is very low:\n",
        "\n",
        "Sparse Graph Structure: The graph captures less local structure, potentially missing important neighborhood information.\n",
        "\n",
        "Loss of Local Structure: Important local relationships are not captured, leading to poor representation of the manifold's geometry.\n",
        "\n",
        "High Sensitivity to Noise: With too few neighbors, the algorithm may become more sensitive to noise and outliers.\n",
        "\n",
        "When Isomap is Superior to PCA\n",
        "\n",
        "Non-linear Relationships: Isomap is designed to handle non-linear relationships between data points, whereas PCA is linear and may not capture the true structure of data lying on a non-linear manifold.\n",
        "\n",
        "Preservation of Geodesic Distances: Isomap preserves the geodesic distances between points, making it suitable for data that lie on a curved manifold. PCA, on the other hand, maximizes variance along linear directions.\n",
        "\n",
        "Other Manifold Learning Methods\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE):\n",
        "\n",
        "Key Difference:\n",
        "t-SNE is specifically designed for visualization in low dimensions and focuses on preserving local structures, unlike Isomap, which preserves geodesic distances. It uses probability distributions to maintain the similarity between points.\n",
        "\n",
        "Local Linear Embedding (LLE):\n",
        "\n",
        "Key Difference: LLE assumes that each data point lies on a locally linear patch of the manifold. It reconstructs the data points using a weighted sum of their neighbors.\n",
        "Laplacian Eigenmaps:\n",
        "\n",
        "Key Difference: It uses the Laplacian matrix to capture the data's structure, often leading to better preserving local and global geometry compared to PCA.\n",
        "\n",
        "Dealing with Missing Data in Manifold Learning\n",
        "Handling missing data effectively is crucial for manifold learning."
      ],
      "metadata": {
        "id": "8yAET8DXDV-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imputation with Mean/Median/Mode:"
      ],
      "metadata": {
        "id": "lmylQLrjEE_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Example data with missing values\n",
        "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
        "\n",
        "# Using mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "print(imputed_data)\n"
      ],
      "metadata": {
        "id": "A_8msghdEHKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. K-Nearest Neighbors (KNN) Imputation:"
      ],
      "metadata": {
        "id": "FGNkmNKhENmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "# Example data with missing values\n",
        "data = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
        "\n",
        "# Using KNN imputation\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "print(imputed_data)\n"
      ],
      "metadata": {
        "id": "IhRmEE42EP25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using Algorithms with Built-in Handling for Missing Data:\n",
        "\n",
        "Some manifold learning algorithms, like sklearn.manifold.TSNE, can handle missing data internally by specifying appropriate parameters.\n",
        "\n",
        "4. Data Augmentation and Regularization:\n",
        "\n",
        "Use techniques like data augmentation to increase the robustness of your manifold learning model to missing data.\n",
        "\n",
        "By incorporating these strategies, we can handle missing data effectively, ensuring that manifold learning algorithms perform well."
      ],
      "metadata": {
        "id": "Vcg9dFosETlV"
      }
    }
  ]
}