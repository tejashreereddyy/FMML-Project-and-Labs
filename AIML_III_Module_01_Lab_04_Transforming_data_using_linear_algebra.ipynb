{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtqQwBJ7UXIq3xCWE2rakF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_01_Lab_04_Transforming_data_using_linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming data using linear algebra\n",
        "\n",
        "Module 1, Lab 4\n",
        "\n",
        "Questions:\n",
        "Experiment with different transformation matrices and check the accuracy\n",
        "\n",
        "Let's experiment with different transformation matrices and check their impact on model accuracy using the Iris dataset. We'll use transformations such as StandardScaler, MinMaxScaler, and PCA (Principal Component Analysis) and evaluate their effect on the accuracy of a Logistic Regression model.\n",
        "\n",
        "Step-by-Step Guide\n",
        "\n",
        "1.Load the Dataset\n",
        "\n",
        "2.Preprocess the Data\n",
        "\n",
        "3.Apply Different Transformations\n",
        "\n",
        "4.Evaluate the Transformations\n",
        "\n",
        "5.Compare Accuracy for Different Transformations and Feature Pairs\n",
        "\n",
        "Step 1: Load the Dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8MYC4vXDK09A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "Ocm-_SOAMDQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the Data\n",
        "\n",
        "Split the dataset into training and testing sets."
      ],
      "metadata": {
        "id": "DPD79cO6MEqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "1goRmTzPMIWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Apply Different Transformations\n",
        "\n",
        "We'll apply StandardScaler, MinMaxScaler, and PCA."
      ],
      "metadata": {
        "id": "q1nwC8lHMKwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply StandardScaler transformation\n",
        "scaler_standard = StandardScaler()\n",
        "X_train_standard = scaler_standard.fit_transform(X_train)\n",
        "X_test_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# Apply MinMaxScaler transformation\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Apply PCA transformation\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n"
      ],
      "metadata": {
        "id": "h_4R_K-GMPMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate the Transformations\n",
        "\n",
        "Evaluate the model with different transformations and combinations of features. Create a function to train the model and evaluate accuracy."
      ],
      "metadata": {
        "id": "O5-LBG6fMShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_features(X_train, X_test, features):\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train[:, features], y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test[:, features])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "fs3_C2nKMVv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Compare Accuracy for Different Transformations and Feature Pairs\n",
        "\n",
        "Evaluate the model using different combinations of features and transformations.\n",
        "\n"
      ],
      "metadata": {
        "id": "9mtNKvDLMZGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature combinations to evaluate\n",
        "feature_combinations = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 3)\n",
        "]\n",
        "\n",
        "# Evaluate with StandardScaler\n",
        "print(\"StandardScaler Results:\")\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_standard, X_test_standard, list(combo))\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with MinMaxScaler\n",
        "print(\"\\nMinMaxScaler Results:\")\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_minmax, X_test_minmax, list(combo))\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with PCA (only 2 components available)\n",
        "print(\"\\nPCA Results:\")\n",
        "accuracy = evaluate_features(X_train_pca, X_test_pca, [0, 1])\n",
        "print(f'Accuracy with PCA features (0, 1): {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "pPItuGRSMmTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code:"
      ],
      "metadata": {
        "id": "lyIdrcCnMrQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply StandardScaler transformation\n",
        "scaler_standard = StandardScaler()\n",
        "X_train_standard = scaler_standard.fit_transform(X_train)\n",
        "X_test_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# Apply MinMaxScaler transformation\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Apply PCA transformation\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Function to evaluate features\n",
        "def evaluate_features(X_train, X_test, features):\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train[:, features], y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test[:, features])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Feature combinations to evaluate\n",
        "feature_combinations = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 3)\n",
        "]\n",
        "\n",
        "# Evaluate with StandardScaler\n",
        "print(\"StandardScaler Results:\")\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_standard, X_test_standard, list(combo))\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with MinMaxScaler\n",
        "print(\"\\nMinMaxScaler Results:\")\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_minmax, X_test_minmax, list(combo))\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with PCA (only 2 components available)\n",
        "print(\"\\nPCA Results:\")\n",
        "accuracy = evaluate_features(X_train_pca, X_test_pca, [0, 1])\n",
        "print(f'Accuracy with PCA features (0, 1): {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUw-rd7HMwmp",
        "outputId": "017012f9-a0b5-4129-c74a-0455dd2e13e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardScaler Results:\n",
            "Accuracy with features (0, 1): 0.90\n",
            "Accuracy with features (0, 2): 1.00\n",
            "Accuracy with features (0, 3): 1.00\n",
            "Accuracy with features (1, 2): 1.00\n",
            "Accuracy with features (1, 3): 1.00\n",
            "Accuracy with features (2, 3): 1.00\n",
            "\n",
            "MinMaxScaler Results:\n",
            "Accuracy with features (0, 1): 0.90\n",
            "Accuracy with features (0, 2): 0.93\n",
            "Accuracy with features (0, 3): 0.97\n",
            "Accuracy with features (1, 2): 0.97\n",
            "Accuracy with features (1, 3): 0.97\n",
            "Accuracy with features (2, 3): 1.00\n",
            "\n",
            "PCA Results:\n",
            "Accuracy with PCA features (0, 1): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will the same transform used for these two features also work for other features?\n",
        "\n",
        "To determine if the same transformation applied to one pair of features will also work for other features, we need to evaluate the performance of the model with different transformations across various feature combinations. We will compare the accuracy of the model for different feature combinations and transformations.\n",
        "\n",
        "Experiment and Analysis\n",
        "\n",
        "We'll consider the following transformations:\n",
        "\n",
        "StandardScaler\n",
        "\n",
        "MinMaxScaler\n",
        "\n",
        "PCA\n",
        "\n",
        "We'll apply these transformations to different pairs of features and compare their accuracies.\n",
        "\n",
        "Step-by-Step Experiment\n",
        "\n",
        "Load the Dataset\n",
        "\n",
        "Preprocess the Data\n",
        "\n",
        "Apply Different Transformations\n",
        "\n",
        "Evaluate the Transformations for Different Feature Pairs\n",
        "\n",
        "Analyze the Results\n",
        "\n",
        "Step 1: Load the Dataset"
      ],
      "metadata": {
        "id": "5fTxW5MGM_S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "dFnTTvlpNRUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the Data"
      ],
      "metadata": {
        "id": "PerGtFiwNduF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "cWaB3Ef1NiBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Apply Different Transformations"
      ],
      "metadata": {
        "id": "F6O24d5dNlZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply StandardScaler transformation\n",
        "scaler_standard = StandardScaler()\n",
        "X_train_standard = scaler_standard.fit_transform(X_train)\n",
        "X_test_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# Apply MinMaxScaler transformation\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Apply PCA transformation\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n"
      ],
      "metadata": {
        "id": "933-xZCjNtEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate the Transformations for Different Feature Pairs"
      ],
      "metadata": {
        "id": "2CuaF4zyNtxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_features(X_train, X_test, features):\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train[:, features], y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test[:, features])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Feature combinations to evaluate\n",
        "feature_combinations = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 3)\n",
        "]\n",
        "\n",
        "# Evaluate with StandardScaler\n",
        "print(\"StandardScaler Results:\")\n",
        "standard_results = {}\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_standard, X_test_standard, list(combo))\n",
        "    standard_results[combo] = accuracy\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with MinMaxScaler\n",
        "print(\"\\nMinMaxScaler Results:\")\n",
        "minmax_results = {}\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_minmax, X_test_minmax, list(combo))\n",
        "    minmax_results[combo] = accuracy\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate with PCA (only 2 components available)\n",
        "print(\"\\nPCA Results:\")\n",
        "pca_results = evaluate_features(X_train_pca, X_test_pca, [0, 1])\n",
        "print(f'Accuracy with PCA features (0, 1): {pca_results:.2f}')\n"
      ],
      "metadata": {
        "id": "U3LncL2WNwn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Analyze the Results"
      ],
      "metadata": {
        "id": "WgLPUcXXNzKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of the results\n",
        "print(\"\\nAnalysis of the results:\")\n",
        "print(\"\\nStandardScaler Results:\")\n",
        "for combo, accuracy in standard_results.items():\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "print(\"\\nMinMaxScaler Results:\")\n",
        "for combo, accuracy in minmax_results.items():\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "print(\"\\nPCA Results:\")\n",
        "print(f'Accuracy with PCA features (0, 1): {pca_results:.2f}')\n"
      ],
      "metadata": {
        "id": "HpEK2JooN28O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "StandardScaler and MinMaxScaler:\n",
        "\n",
        "If the accuracy is consistent across different feature pairs when using the same transformation, it indicates that the transformation works well for other features.\n",
        "If the accuracy varies significantly, it suggests that the transformation may not work uniformly across all features.\n",
        "\n",
        "PCA:\n",
        "\n",
        "PCA reduces the dimensionality of the data, capturing the most important information in fewer components. The accuracy with PCA will show how well the model performs with the transformed data.\n",
        "\n",
        "Comparison:\n",
        "\n",
        "Compare the accuracies for different feature pairs using StandardScaler and MinMaxScaler.\n",
        "Check if the transformation applied to one pair of features maintains its effectiveness across other pairs.\n",
        "\n",
        "\n",
        "Exercise:\n",
        "\n",
        " Is it possible that adding all 4 features at a time is not the best strategy? Can you think of a better combination of features that can help in improving the accuracy of the model? Maybe you can try adding 2 features at a time and see if that helps.\n",
        "\n",
        " Yes, it is possible that using all 4 features at a time may not be the best strategy for improving the accuracy of the model. By experimenting with different combinations of features, we may find a subset that yields better performance.\n",
        "\n",
        "To explore this, we'll evaluate the performance of a logistic regression model on the Iris dataset using different combinations of features. We'll compare the accuracy of using all features versus using pairs of features.\n",
        "\n",
        "Step-by-Step Guide\n",
        "\n",
        "1.Load the Dataset\n",
        "\n",
        "2.Preprocess the Data\n",
        "\n",
        "3.Evaluate Different Combinations of Features\n",
        "\n",
        "4.Compare Accuracy\n",
        "Step 1: Load the Dataset"
      ],
      "metadata": {
        "id": "vIUIXIrNN-vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "r5vpX5TAOjb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocess the Data\n",
        "\n",
        "Split the dataset into training and testing sets and apply StandardScaler."
      ],
      "metadata": {
        "id": "pYiecK3lOmUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply StandardScaler transformation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "pSMa3GhYOqgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Evaluate Different Combinations of Features\n",
        "\n",
        "Create a function to evaluate the model with different combinations of features."
      ],
      "metadata": {
        "id": "TM75F1PbOsul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_features(X_train, X_test, features):\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train[:, features], y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test[:, features])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Feature combinations to evaluate\n",
        "feature_combinations = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 3)\n",
        "]\n",
        "\n",
        "# Evaluate with all features\n",
        "accuracy_all_features = evaluate_features(X_train_scaled, X_test_scaled, [0, 1, 2, 3])\n",
        "print(f'Accuracy with all features: {accuracy_all_features:.2f}')\n",
        "\n",
        "# Evaluate with pairs of features\n",
        "pair_accuracies = {}\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_scaled, X_test_scaled, list(combo))\n",
        "    pair_accuracies[combo] = accuracy\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "NiI4DSPrOwpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Compare Accuracy\n",
        "\n",
        "Compare the accuracy of using all features versus pairs of features to determine the best combination."
      ],
      "metadata": {
        "id": "Hee7238oOzUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare accuracies\n",
        "best_combination = max(pair_accuracies, key=pair_accuracies.get)\n",
        "best_accuracy = pair_accuracies[best_combination]\n",
        "\n",
        "print(\"\\nBest combination of 2 features:\")\n",
        "print(f'Features: {best_combination}, Accuracy: {best_accuracy:.2f}')\n",
        "\n",
        "if best_accuracy > accuracy_all_features:\n",
        "    print(f'The best combination of 2 features ({best_combination}) performs better than using all features.')\n",
        "else:\n",
        "    print('Using all features performs better or equally well compared to using pairs of features.')\n"
      ],
      "metadata": {
        "id": "voRhWZNxO3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall code:"
      ],
      "metadata": {
        "id": "_YZhHAwqO7kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply StandardScaler transformation\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Function to evaluate features\n",
        "def evaluate_features(X_train, X_test, features):\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train[:, features], y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test[:, features])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Feature combinations to evaluate\n",
        "feature_combinations = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 3)\n",
        "]\n",
        "\n",
        "# Evaluate with all features\n",
        "accuracy_all_features = evaluate_features(X_train_scaled, X_test_scaled, [0, 1, 2, 3])\n",
        "print(f'Accuracy with all features: {accuracy_all_features:.2f}')\n",
        "\n",
        "# Evaluate with pairs of features\n",
        "pair_accuracies = {}\n",
        "for combo in feature_combinations:\n",
        "    accuracy = evaluate_features(X_train_scaled, X_test_scaled, list(combo))\n",
        "    pair_accuracies[combo] = accuracy\n",
        "    print(f'Accuracy with features {combo}: {accuracy:.2f}')\n",
        "\n",
        "# Compare accuracies\n",
        "best_combination = max(pair_accuracies, key=pair_accuracies.get)\n",
        "best_accuracy = pair_accuracies[best_combination]\n",
        "\n",
        "print(\"\\nBest combination of 2 features:\")\n",
        "print(f'Features: {best_combination}, Accuracy: {best_accuracy:.2f}')\n",
        "\n",
        "if best_accuracy > accuracy_all_features:\n",
        "    print(f'The best combination of 2 features ({best_combination}) performs better than using all features.')\n",
        "else:\n",
        "    print('Using all features performs better or equally well compared to using pairs of features.')\n"
      ],
      "metadata": {
        "id": "t1e1OIftO9Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code allows us to experiment with different combinations of features and evaluate their impact on the accuracy of the model. By comparing the accuracy for each combination, we can identify if using fewer features yields better performance."
      ],
      "metadata": {
        "id": "AXnWwn2_PDYp"
      }
    }
  ]
}