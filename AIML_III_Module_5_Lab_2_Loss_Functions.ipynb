{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX2/PwrbUIjbdQIsDASv+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_5_Lab_2_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Students Internship Batch of 2024**\n",
        "#Regression Lab 2: Loss Functions"
      ],
      "metadata": {
        "id": "U4tzlNtP0h-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out these loss functions on regression tasks, and try to understand the model performance based on the loss function.\n",
        "\n",
        "Explore other loss functions and try to understand when and why they are used.\n",
        "\n",
        "Try out KNN-regression on other datasets see which values of K give the best results.\n",
        "\n",
        "Try exploring what will happen if K is set to 1 or K is set to size of whole training dataset. Give me example codes"
      ],
      "metadata": {
        "id": "v48UKUyG952q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared Error\n",
        "\n",
        "As we have seen before, the formula for MSE is\n",
        "\n",
        "$MSE = \\frac{1}{m} \\sum_{i=1}^n (y-y_p)^2$\n",
        "\n",
        "The image below depicts a visualization of what the squared error is.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12jmqRrrqknIWKhwRpgRIJAErBjkEcyx9\" width=\"350\" height=\"350\" align=\"middle\"/>\n",
        "\n",
        "MSE is also referred to as L2 loss.\n",
        "\n",
        "Typically, we would want the units of error to be the same as the data, so we use Root Mean Squared Error instead.\n",
        "\n",
        "### The concept of maximum likelihood\n",
        "\n",
        "Ideally, we would want to obtain a model $\\hat{\\theta}$ which maximizes the probability of correctly predicting data. Mathematically, the likelihood is:\n",
        "\n",
        "$ \\prod_{i=1}^{N}  \\: \\hat{\\theta_i}^{\\theta_i}$\n",
        "\n",
        "This is because our actual data ($\\theta_i$) is either 0 or 1 depending on the label of the data, and our predicted data ($\\hat{\\theta_i}$) is a probability.\n",
        "\n",
        "We usually consider the negative log of the likelihood, since log is monotonic and easier to deal with as an optimization problem.\n",
        "\n",
        "Hence, we have\n",
        "\n",
        "$MLE = $ arg min $\\: - \\sum_{i=1}^{N} \\theta_i \\: log \\: \\hat{\\theta_i}$\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "## Entropy and Cross-Entropy\n",
        "\n",
        "For some event $x$, we can obtain **information** based on its probability distribution. For example, for some event that occurs with $p=1$, we gain no information. Now, if we flip a coin and see that we obtained heads, we say that we got 1 bit of information.\n",
        "\n",
        "Thus, the information $I$ of some event $x$ is:\n",
        "\n",
        "$I = -log_2 \\: p(x)$\n",
        "\n",
        "We say that events with a low probability of occuring give high information, and those with a low probability give low information.\n",
        "\n",
        "Now, if we have some random variable $X$, its **entropy** is the expected value of the information obtained.\n",
        "\n",
        "$H(x) = - \\sum_{k} p_k \\: log \\: p_k$\n",
        "\n",
        "Now, suppose we are trying to correctly predict labels of some data. Let P be the true distribution of the labels, and Q be the predicted distribution of labels.\n",
        "\n",
        "Cross-Entropy is then defined as:\n",
        "\n",
        "$H(P,Q) = - \\sum_{x} P(x)\\: log \\: Q(x)$\n",
        "\n",
        "which is basically the entropy between two probability distributions over the same events.\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "## MLE and Cross-Entropy\n",
        "\n",
        "So, in the case of classification, the equation for minimization of the cross-entropy between the actual data and the predicted data would be:\n",
        "\n",
        "arg min $\\: -\\sum_{i=1}^{N} P(x) \\: log \\: Q(x)$\n",
        "\n",
        "$= $ arg min $\\: -\\sum_{i=1}^{N} \\theta_i \\: log \\: \\hat{\\theta_i}$\n",
        "\n",
        "\n",
        "which is exactly the same as what we had obtained for minimizing the negative log likelihood. Hence, the two problems are equivalent."
      ],
      "metadata": {
        "id": "0T03hnHQ-FCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "data['MedHouseVal'] = housing.target  # Target variable\n",
        "\n",
        "# Features and target variable\n",
        "X = data.drop('MedHouseVal', axis=1).values\n",
        "y = data['MedHouseVal'].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Exploring different values of K\n",
        "for k in [1, 5, 10, len(X_train)]:\n",
        "    # KNN Regressor\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    # Calculate loss functions\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    print(f'For K={k}:')\n",
        "    print(f'MSE: {mse}')\n",
        "    print(f'MAE: {mae}')\n",
        "    print('-' * 30)\n",
        "\n",
        "# Example for tuning K with cross-validation using GridSearchCV\n",
        "param_grid = {'n_neighbors': np.arange(1, 20)}\n",
        "knn = KNeighborsRegressor()\n",
        "\n",
        "grid_search = GridSearchCV(knn, param_grid, scoring=make_scorer(mean_squared_error, greater_is_better=False), cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "print(f'Best K value: {best_k}')\n",
        "\n",
        "# Train KNN with the best K\n",
        "knn_best = KNeighborsRegressor(n_neighbors=best_k)\n",
        "knn_best.fit(X_train, y_train)\n",
        "y_pred_best = knn_best.predict(X_test)\n",
        "\n",
        "# Calculate loss functions for the best model\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
        "\n",
        "print(f'Best Model Performance:')\n",
        "print(f'MSE: {mse_best}')\n",
        "print(f'MAE: {mae_best}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYk1q_Qo-8k4",
        "outputId": "f9e8b2bb-635e-4a37-dca3-e6a0d950da2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For K=1:\n",
            "MSE: 0.6159512624411578\n",
            "MAE: 0.5172981371124031\n",
            "------------------------------\n",
            "For K=5:\n",
            "MSE: 0.4044538375892015\n",
            "MAE: 0.430758769379845\n",
            "------------------------------\n",
            "For K=10:\n",
            "MSE: 0.3909628171015683\n",
            "MAE: 0.42732068023255815\n",
            "------------------------------\n",
            "For K=16512:\n",
            "MSE: 1.3043431479307854\n",
            "MAE: 0.9071315345587282\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K value: 10\n",
            "Best Model Performance:\n",
            "MSE: 0.3909628171015683\n",
            "MAE: 0.42732068023255815\n"
          ]
        }
      ]
    }
  ]
}