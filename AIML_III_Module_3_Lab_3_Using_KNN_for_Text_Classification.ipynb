{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Jt8qugt5dV5Tc4l5Z1uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/AIML_III_Module_3_Lab_3_Using_KNN_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Students Internship Batch of 2024**\n",
        "### MODULE: CLASSIFICATION-1\n",
        "### LAB-3 : Using KNN for Text Classification\n",
        "\n",
        "\n",
        "---\n",
        "### Why TF-IDF Generally Results in Better Accuracy Than Bag-of-Words (BoW)\n",
        "\n",
        "1. **Weighting Scheme**:\n",
        "   - **Bag-of-Words (BoW)** simply counts the occurrence of words in a document without considering the importance of a word in the context of a document collection. This often results in common words like \"the,\" \"is,\" and \"in\" having the same weight as more informative words.\n",
        "   - **TF-IDF** (Term Frequency-Inverse Document Frequency) addresses this by giving more weight to words that are frequent in a document but not frequent across all documents. This reduces the influence of common words and highlights more relevant terms.\n",
        "\n",
        "2. **Reducing the Impact of Common Words**:\n",
        "   - TF-IDF reduces the weight of very common words that are not informative for distinguishing between documents. BoW, on the other hand, treats all words equally, which can result in less accurate models because of the noise introduced by these common words.\n",
        "\n",
        "3. **Discrimination Power**:\n",
        "   - TF-IDF can better capture the uniqueness of a document by emphasizing words that help distinguish it from others, which often leads to better performance in tasks like text classification and information retrieval.\n",
        "\n",
        "### Techniques Better Than Both BoW and TF-IDF\n",
        "\n",
        "1. **Word Embeddings (e.g., Word2Vec, GloVe, FastText)**:\n",
        "   - These techniques represent words in continuous vector spaces where semantically similar words are closer together. They capture more nuanced relationships between words, such as context and semantics, which BoW and TF-IDF do not.\n",
        "\n",
        "2. **Document Embeddings (e.g., Doc2Vec)**:\n",
        "   - Document embeddings extend word embeddings to entire documents, allowing for better representation of the document as a whole, capturing both syntax and semantics more effectively than BoW or TF-IDF.\n",
        "\n",
        "3. **Transformer-Based Models (e.g., BERT, GPT)**:\n",
        "   - These models capture context and semantics at a very deep level by considering the entire sequence of words in a document. They can understand context better than BoW and TF-IDF, leading to significantly better performance in a wide range of NLP tasks.\n",
        "\n",
        "4. **Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)**:\n",
        "   - These techniques capture the underlying structure of the document corpus by reducing the dimensionality of the term-document matrix. LSA captures the relationships between terms by mapping them to a latent space, while LDA models topics as distributions over words, providing better interpretability and performance in some cases.\n",
        "\n",
        "### Stemming vs. Lemmatization: Pros and Cons\n",
        "\n",
        "**Stemming**:\n",
        "- **Pros**:\n",
        "  - Simple and fast: Stemming involves basic rules-based chopping of word endings, making it computationally inexpensive.\n",
        "  - Effective in reducing dimensionality: By reducing words to their base form, stemming can help decrease the number of unique words, simplifying the model.\n",
        "  \n",
        "- **Cons**:\n",
        "  - Crude: Stemming can sometimes produce non-meaningful stems (e.g., \"running\" to \"run\" vs. \"fishing\" to \"fish\").\n",
        "  - Lack of context: Stemming does not consider the context or the actual meaning of the word, which can lead to inaccurate reductions (e.g., \"better\" to \"bett\").\n",
        "\n",
        "**Lemmatization**:\n",
        "- **Pros**:\n",
        "  - Context-aware: Lemmatization considers the context of the word and its meaning, producing more accurate and meaningful base forms (e.g., \"better\" to \"good\").\n",
        "  - Improved text processing: Because it returns valid words, lemmatization often leads to better results in NLP tasks.\n",
        "\n",
        "- **Cons**:\n",
        "  - Slower and more complex: Lemmatization requires more computational resources and often relies on a dictionary or linguistic rules.\n",
        "  - Might not reduce dimensionality as much: Since it doesnâ€™t reduce words to a common form as aggressively as stemming, it might not lower the dimensionality of the term space as effectively.\n",
        "\n",
        "In practice, the choice between stemming and lemmatization depends on the specific requirements of the task and the importance of preserving the meaning of words in the text processing pipeline."
      ],
      "metadata": {
        "id": "m-umN3so5h3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SeBIJ7qo7J-h"
      }
    }
  ]
}