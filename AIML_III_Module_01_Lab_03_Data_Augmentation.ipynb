{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpBktUlS5SGLcnTeZHxsG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejashreereddyy/FMML-Project-and-Labs/blob/main/Copy_of_AIML_III_Module_01_Lab_03_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation\n",
        "\n",
        "Module 1, Lab 3\n",
        "\n",
        "Data augmentation is a technique used to increase the diversity of your training data without collecting new data. By applying random transformations such as rotation and shear, we can make our model more robust and improve its generalization performance.\n",
        "\n",
        "Rotation: This transformation rotates the image by a random angle within a specified range. For instance, setting the rotation range to 20 means the image will be rotated by a random angle between -20 and 20 degrees.\n",
        "\n",
        "Shear: This transformation shears the image by a random factor within a specified range. Shearing means tilting the image in a way that some parts of it are pushed sideways, creating a sort of slanted look.\n",
        "\n",
        "Questions\n",
        "\n",
        "1. What is the best value for angle constraint and shear constraint you got? How much did the accuracy improve as compared to not using augmentations?\n",
        "\n",
        "We need to perform a grid search over different values of rotation and shear to find the best combination."
      ],
      "metadata": {
        "id": "uiXPkLgcyTTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define a simple CNN model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the range of values for rotation and shear\n",
        "angle_range = np.arange(0, 91, 10)\n",
        "shear_range = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "# Placeholder for results\n",
        "results = []\n",
        "\n",
        "# Grid search over the range of values\n",
        "for angle in angle_range:\n",
        "    for shear in shear_range:\n",
        "        # Data augmentation with current parameters\n",
        "        datagen = ImageDataGenerator(rotation_range=angle, shear_range=shear)\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = create_model()\n",
        "        history = model.fit(datagen.flow(x_train, y_train, batch_size=128),\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            epochs=10, verbose=0)\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "        results.append((angle, shear, accuracy))\n",
        "        print(f\"Angle: {angle}, Shear: {shear}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Find the best parameters\n",
        "best_params = max(results, key=lambda x: x[2])\n",
        "print(f\"Best parameters - Angle: {best_params[0]}, Shear: {best_params[1]}, Accuracy: {best_params[2]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0_7dY9lzX-r",
        "outputId": "89e83a17-084b-4a86-e7ba-fb22b1f297e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Comparing with No Augmentation\n",
        "\n",
        "Train a model without any augmentation and measure the accuracy."
      ],
      "metadata": {
        "id": "MOzwzvjTzgi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train without augmentation\n",
        "model_no_aug = create_model()\n",
        "history_no_aug = model_no_aug.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=0)\n",
        "_, accuracy_no_aug = model_no_aug.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Accuracy without augmentation: {accuracy_no_aug:.4f}\")\n"
      ],
      "metadata": {
        "id": "uZwZ-clAziP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Increasing Number of Augmentations per Sample\n",
        "\n",
        "Evaluate the impact of increasing the number of augmentations."
      ],
      "metadata": {
        "id": "Mu-O5_e5zmkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of augmentations per sample\n",
        "augmentations = [1, 2, 5, 10]\n",
        "results_augmentations = []\n",
        "\n",
        "for num_aug in augmentations:\n",
        "    datagen = ImageDataGenerator(rotation_range=best_params[0], shear_range=best_params[1])\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Create augmented dataset\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "    for _ in range(num_aug):\n",
        "        for x, y in datagen.flow(x_train, y_train, batch_size=len(x_train)):\n",
        "            augmented_data.append(x)\n",
        "            augmented_labels.append(y)\n",
        "            break\n",
        "    augmented_data = np.vstack(augmented_data)\n",
        "    augmented_labels = np.vstack(augmented_labels)\n",
        "\n",
        "    # Train the model\n",
        "    model_aug = create_model()\n",
        "    history_aug = model_aug.fit(augmented_data, augmented_labels, validation_data=(x_test, y_test), epochs=10, verbose=0)\n",
        "    _, accuracy_aug = model_aug.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    results_augmentations.append((num_aug, accuracy_aug))\n",
        "    print(f\"Number of augmentations: {num_aug}, Accuracy: {accuracy_aug:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(augmentations, [x[1] for x in results_augmentations])\n",
        "plt.xlabel('Number of Augmentations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Number of Augmentations')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UkxnX6bqzqYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implementing Custom Augmentations\n",
        "\n",
        "Implement a few custom augmentations and experiment."
      ],
      "metadata": {
        "id": "abAPO82Qzt5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom augmentations (example: horizontal flip, brightness adjustment)\n",
        "datagen_custom = ImageDataGenerator(rotation_range=best_params[0],\n",
        "                                    shear_range=best_params[1],\n",
        "                                    horizontal_flip=True,\n",
        "                                    brightness_range=[0.8, 1.2])\n",
        "\n",
        "datagen_custom.fit(x_train)\n",
        "\n",
        "# Train the model with custom augmentations\n",
        "model_custom = create_model()\n",
        "history_custom = model_custom.fit(datagen_custom.flow(x_train, y_train, batch_size=128),\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  epochs=10, verbose=0)\n",
        "_, accuracy_custom = model_custom.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Accuracy with custom augmentations: {accuracy_custom:.4f}\")\n"
      ],
      "metadata": {
        "id": "vb8yusLBzxfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Combining Various Augmentations\n",
        "\n",
        "Combine different augmentations to find the highest accuracy."
      ],
      "metadata": {
        "id": "WxifTJSEzz61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined augmentations\n",
        "datagen_combined = ImageDataGenerator(rotation_range=best_params[0],\n",
        "                                      shear_range=best_params[1],\n",
        "                                      horizontal_flip=True,\n",
        "                                      zoom_range=0.2,\n",
        "                                      brightness_range=[0.8, 1.2])\n",
        "\n",
        "datagen_combined.fit(x_train)\n",
        "\n",
        "# Train the model with combined augmentations\n",
        "model_combined = create_model()\n",
        "history_combined = model_combined.fit(datagen_combined.flow(x_train, y_train, batch_size=128),\n",
        "                                      validation_data=(x_test, y_test),\n",
        "                                      epochs=10, verbose=0)\n",
        "_, accuracy_combined = model_combined.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Accuracy with combined augmentations: {accuracy_combined:.4f}\")\n"
      ],
      "metadata": {
        "id": "0_RNQoYEz2zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Smallest Training Dataset for 50% Accuracy\n",
        "\n",
        "Determine the smallest training dataset size that achieves accuracy above 50%."
      ],
      "metadata": {
        "id": "_B-3LhCtz7Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce training dataset sizes\n",
        "dataset_sizes = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "results_dataset_sizes = []\n",
        "\n",
        "for size in dataset_sizes:\n",
        "    subset_x_train = x_train[:int(len(x_train) * size)]\n",
        "    subset_y_train = y_train[:int(len(y_train) * size)]\n",
        "\n",
        "    # Train the model\n",
        "    model_subset = create_model()\n",
        "    history_subset = model_subset.fit(subset_x_train, subset_y_train, validation_data=(x_test, y_test), epochs=10, verbose=0)\n",
        "    _, accuracy_subset = model_subset.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    results_dataset_sizes.append((size, accuracy_subset))\n",
        "    print(f\"Dataset size: {size * 100:.1f}%, Accuracy: {accuracy_subset:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.plot([size * 100 for size in dataset_sizes], [x[1] for x in results_dataset_sizes])\n",
        "plt.xlabel('Training Dataset Size (%)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Training Dataset Size')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TcWid0Wc0AsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "> Try to take 50 images of each digit and calculate the performance on test set.\n"
      ],
      "metadata": {
        "id": "KlIzPxUA0EQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take 50 images of each digit\n",
        "x_train_50 = []\n",
        "y_train_50 = []\n",
        "\n",
        "for digit in range(10):\n",
        "    indices = np.where(y_train.argmax(axis=1) == digit)[0][:50]\n",
        "    x_train_50.append(x_train[indices])\n",
        "    y_train_50.append(y_train[indices])\n",
        "\n",
        "x_train_50 = np.vstack(x_train_50)\n",
        "y_train_50 = np.vstack(y_train_50)\n",
        "\n",
        "# Data augmentation with best parameters\n",
        "datagen_50 = ImageDataGenerator(rotation_range=best_params[0], shear_range=best_params[1])\n",
        "datagen_50.fit(x_train_50)\n",
        "\n",
        "# Train the model with 50 images per digit\n",
        "model_50 = create_model()\n",
        "history_50 = model_50.fit(datagen_50.flow(x_train_50, y_train_50, batch_size=128),\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          epochs=10, verbose=0)\n",
        "_, accuracy_50 = model_50.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Accuracy with 50 images per digit: {accuracy_50:.4f}\")\n"
      ],
      "metadata": {
        "id": "GA_knjtC0P46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Best values for angle and shear constraints were found using a grid search.\n",
        "\n",
        "Accuracy improved compared to no augmentations.\n",
        "\n",
        "Increasing the number of augmentations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7w4mcqiV0Tu6"
      }
    }
  ]
}
